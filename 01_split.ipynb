{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we study the internal mechanism used by decision tree to make some classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a dataset made of 200 samples and a single feature. These samples are equally split into 2 classes following some normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class_samples = 100\n",
    "X = np.hstack(\n",
    "    [rng.randn(n_class_samples), 2.5 + rng.randn(n_class_samples)]\n",
    ").T\n",
    "y = np.array([0] * n_class_samples + [1] * n_class_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease some later processing, we are sorting our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = np.argsort(X)\n",
    "X = X[sorted_idx]\n",
    "y = y[sorted_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can give a look at the class distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clazz in np.unique(y):\n",
    "    plt.hist(X[y==clazz], alpha=0.7, label=f'Class #{clazz}', density=True)\n",
    "plt.legend()\n",
    "plt.xlabel('Feature value')\n",
    "plt.ylabel('Class probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have only few samples, we can use a `swarmplot` to visualize all samples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'data': X, 'label': y, 'feature': \"\"})\n",
    "_ = sns.swarmplot(x='data', y='feature', hue='label', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of some concepts: probabilities, entropy and information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees for classification are based on the following concepts: probabilities, entropy, and information gain. We present those concepts and describe where they come into play in the classification procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ultimate objective: find the best split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find a threshold which best split the dataset. In other words, we want the boundary such that most samples smaller than this boundary belong to a class and most samples above this boundary belong to the counterpart class. Let's visualise what we mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = rng.choice(np.arange(y.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.swarmplot(x='data', y='feature', hue='label', data=df)\n",
    "_ = ax.plot([X[random_idx], X[random_idx]], [-1, 1], '-.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we expect as much as possible blue samples on the left of the decision boundary and as much as possible orange samples on the right of the decision boundary.\n",
    "\n",
    "Thus, we need to compute some statistics characteristic the quality of this split, sometimes called impurity. Let's group the labels into two groups, all labels corresponding to the samples on the left of the boundary and all labels corresponding to the samples on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>GET SOME INTUITIONS</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Which quantity could you compute for the left and right side of the split to indicate their purity?\n",
    "      </li>\n",
    "      <li>\n",
    "      How could you combine the parent, left, and right purity to have an overall indication of the split quality?\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_left = y[:random_idx]\n",
    "labels_right = y[random_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impurity is related to the number of samples of each class on a side of the boundary. We can compute the probability which is the number of samples of a class on the total number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def compute_probability(labels):\n",
    "    counter = Counter(labels)\n",
    "    n_total_samples = sum(counter.values())\n",
    "    probabilities = {}\n",
    "    \n",
    "    for clazz, count_clazz in counter.items():\n",
    "        probabilities[clazz] = count_clazz / n_total_samples\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for the previous split, we could compute the probabilities for each class on the left and right side of the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_left = compute_probability(labels_left)\n",
    "prob_right = compute_probability(labels_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Probability on the left split {prob_left}')\n",
    "print(f'Probability on the right split {prob_right}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results make sense. On the right side of the boundary, there is mainly orange samples meaning a very high probability for class #1 and a small probability for the blue class (class #0).\n",
    "\n",
    "On the right side of the boundary, the probabilities are less extreme with samples belonging to both classes\n",
    "\n",
    "Therefore, we can build the intuition that we need a split which will have a maximum probability for the class #0 on the left side of the split and a maximum probability for the class #1 for the right side of the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we computed probabilities. It gives several statistics depending on the class, and it will be handy to get a single statistic which characterises the impurity of a side of the boundary: we will use the entropy definition.\n",
    "\n",
    "Entropy is defined: $H(X) = - \\sum_{k=1}^{K} p(X_k) \\log_{2} p(X_k)$, where $K$ is the number of classes. In practise the entropy function looks like:\n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/2/22/Binary_entropy_plot.svg)\n",
    "\n",
    "Considering some labels, the entropy will be maximum if there are as many samples from one class than another. It will be minimum when there are only samples from a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    probabilities = compute_probability(labels)\n",
    "\n",
    "    H_X = 0\n",
    "    for clazz in probabilities:\n",
    "        H_X -= probabilities[clazz] * np.log2(probabilities[clazz])\n",
    "\n",
    "    return H_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the entropy for each side of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_left = entropy(labels_left)\n",
    "entropy_right = entropy(labels_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Entropy on the left split {entropy_left}')\n",
    "print(f'Entropy on the right split {entropy_right}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the entropy on the right side is minimum. We only had orange samples on that side. The entropy of the right side is closer to one because we have almost half-half orange and blue samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current split, we currently have two statistics, an entropy for each side of the split. It will be handy to get a single statistic to summarise the quality of the split. It is called the information gain.\n",
    "\n",
    "Indeed, the information gain corresponds to the entropy computed before splitting the data to which we subtract the entropy from the right and left side of the split (with some additional normalisation factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(labels_parent, labels_left, labels_right):\n",
    "    entropy_parent = entropy(labels_parent)\n",
    "    entropy_left = entropy(labels_left)\n",
    "    entropy_right = entropy(labels_right)\n",
    "\n",
    "    n_samples_parent = labels_parent.size\n",
    "    n_samples_left = labels_left.size\n",
    "    n_samples_right = labels_right.size\n",
    "\n",
    "    impurity_left = (n_samples_left / n_samples_parent) * entropy_left\n",
    "    impurity_right = (n_samples_right / n_samples_parent) * entropy_right\n",
    "\n",
    "    return entropy_parent - entropy_left - entropy_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain_split = information_gain(y, labels_left, labels_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Information for the current {info_gain_split}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meaning of information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is called information gain because it relates to the gain in \"purity\" obtained by partitioning the data. We can try several random partitions and see that a higher is the gain corresponds to a better split to partition the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = rng.choice(np.arange(y.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_left = y[:random_idx]\n",
    "labels_right = y[random_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.swarmplot(x='data', y='feature', hue='label', data=df)\n",
    "_ = ax.plot([X[random_idx], X[random_idx]], [-1, 1], '-.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain_split = information_gain(y, labels_left, labels_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Information for the current {info_gain_split}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: Greddy search for the maximum information gain</b>:\n",
    "    <br>\n",
    "    Instead of trying to random split, we can use a greedy strategy by iteratively trying all possible value and use the one maximum the information gain.\n",
    "     <ul>\n",
    "      <li>\n",
    "      Iterate over all possible values in `y` and store the information gain;\n",
    "      </li>\n",
    "      <li>\n",
    "      Find the index corresponding to the maximum information gain;\n",
    "      </li>\n",
    "      <li>\n",
    "      Plot the all information gain values for all possible threshold;\n",
    "      </li>\n",
    "      <li>\n",
    "      Plot a marker for the threshold maximizing the information gain;\n",
    "      </li>\n",
    "      <li>\n",
    "      Plot the original swarm plot and superpose the line corresponding to the best threshold found.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE: DecisionTreeClassifier as a decision stump</b>:\n",
    "    <ul>\n",
    "      <li>\n",
    "      Use a `sklearn.tree.DecisionTreeClassifier` to classifiy the data `X` and `y`. Train the decision tree. Ensure to use the good criterion and to limit the depth of the tree to get a similar process than what we did before.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this classifier is trained, we can plot the resulting decision and compare it with the previous procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_ = plot_tree(tree)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
